#!/usr/bin/env python3
"""Quick analysis of CSV tag data with enhanced consolidation."""

import sys
import os
import pandas as pd
from collections import Counter
from pathlib import Path

# Add src to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

from tags.normalizer.tag_normalizer import TagNormalizer

def quick_csv_analysis():
    """Quick analysis of CSV data to show enhanced consolidation impact."""
    
    print("üéØ QUICK ENHANCED CONSOLIDATION ANALYSIS")
    print("=" * 50)
    
    normalizer = TagNormalizer()
    csv_dir = Path("csv")
    all_tags = []
    
    print("üîç Processing CSV files...")
    
    # Process a few key CSV files for demonstration
    test_files = [
        "csv/_r_ProgMetal _ Yearly Albums - 2022 Prog-rock.csv",
        "csv/_r_ProgMetal _ Yearly Albums - 2022 Prog-metal.csv", 
        "csv/_r_ProgMetal _ Yearly Albums - 2023 Prog-rock.csv"
    ]
    
    for csv_file in test_files:
        if not os.path.exists(csv_file):
            continue
            
        try:
            # Find header row
            with open(csv_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            header_row = None
            for i, line in enumerate(lines):
                if 'Artist,Album,Release Date,Length,Genre' in line:
                    header_row = i
                    break
            
            if header_row is None:
                continue
            
            # Read CSV
            df = pd.read_csv(csv_file, skiprows=header_row)
            
            if len(df.columns) < 5:
                continue
                
            genre_column = df.columns[4]  # 5th column
            
            # Extract tags from first 200 rows for quick demo
            tags_processed = 0
            for tags_str in df[genre_column].dropna().head(200):
                if isinstance(tags_str, str) and tags_str.strip():
                    tags = [tag.strip() for tag in tags_str.split(',')]
                    all_tags.extend(tags)
                    tags_processed += 1
            
            print(f"üìÑ {Path(csv_file).name}: {tags_processed} rows processed")
            
        except Exception as e:
            print(f"‚ö†Ô∏è  Error with {csv_file}: {e}")
    
    if not all_tags:
        print("‚ùå No tags found in CSV files")
        return
    
    print(f"\nüìä Found {len(all_tags)} total tag instances")
    
    # Count original vs normalized
    original_counter = Counter(all_tags)
    normalized_tags = [normalizer.normalize(tag) for tag in all_tags]
    normalized_counter = Counter(normalized_tags)
    
    original_unique = len(original_counter)
    normalized_unique = len(normalized_counter)
    reduction = original_unique - normalized_unique
    reduction_pct = (reduction / original_unique) * 100 if original_unique > 0 else 0
    
    print(f"üìà Original unique tags: {original_unique}")
    print(f"üìâ Normalized unique tags: {normalized_unique}")
    print(f"üéØ Reduction: {reduction} tags ({reduction_pct:.1f}%)")
    
    # Show top consolidations
    print(f"\nüîß TOP CONSOLIDATIONS:")
    
    consolidations = {}
    for original, normalized in zip(all_tags, normalized_tags):
        if original != normalized:
            if normalized not in consolidations:
                consolidations[normalized] = set()
            consolidations[normalized].add(original)
    
    # Show consolidations with most variants
    for canonical, variants in sorted(consolidations.items(), 
                                    key=lambda x: len(x[1]), 
                                    reverse=True)[:15]:
        if len(variants) > 1:
            variants_str = "', '".join(sorted(variants))
            frequency = sum(original_counter[v] for v in variants)
            print(f"  ‚Ä¢ '{canonical}' ‚Üê ['{variants_str}'] ({frequency} instances)")
    
    # Show frequency impact
    print(f"\nüìä FREQUENCY IMPACT:")
    
    top_originals = original_counter.most_common(10)
    print("  Original top tags:")
    for tag, count in top_originals:
        normalized = normalizer.normalize(tag)
        if tag != normalized:
            print(f"    {count:4} √ó '{tag}' ‚Üí '{normalized}'")
        else:
            print(f"    {count:4} √ó '{tag}' (unchanged)")
    
    print(f"\n‚úÖ Enhanced consolidation system is working effectively!")
    print(f"   The normalizer now applies advanced consolidation rules")
    print(f"   automatically to reduce tag quantity while preserving meaning.")

if __name__ == "__main__":
    quick_csv_analysis()
